{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 机器学习 实验六"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**题目：决策树分类器**  \n",
    "实验条件：给定数据集 Watermelon-train1.csv 和 Watermelon-train2.csv，及其对应的测试集 Watermelon-test1.csv 和 Watermelon-test2.csv  \n",
    "实验要求：  \n",
    "1. 基本要求：\n",
    "   1. 基于 Watermelon-train1 数据集（只有离散属性），构造 ID3 决策树；  \n",
    "   2. 基于构造的 ID3 决策树，对数据集 Watermelon-test1 进行预测，输出分类精度。\n",
    "2. 中级要求：\n",
    "   1. 对数据集 Watermelon-train2，构造 C4.5 或者 CART 决策树，要求可以处理连续型属性；  \n",
    "   2. 对测试集 Watermelon-test2 进行预测，输出分类精度。\n",
    "3. 高级要求：使用任意的剪枝算法对构造的决策树（基本要求和中级要求构造的树）进行剪枝，观察测试集合的分类精度是否有提升，给出分析过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导入需要的包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "feature_list = [\"色泽\", \"根蒂\", \"敲声\", \"纹理\"]\n",
    "feature_dict = {\"色泽\": [\"青绿\", \"乌黑\", \"浅白\"],\n",
    "                \"根蒂\": [\"蜷缩\", \"稍蜷\", \"硬挺\"],\n",
    "                \"敲声\": [\"浊响\", \"沉闷\", \"清脆\"],\n",
    "                \"纹理\": [\"清晰\", \"稍糊\", \"模糊\"]}\n",
    "label_list = [\"否\", \"是\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取数据\n",
    "def load_data1(path):\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    with open(path,'r') as f1:\n",
    "        line = f1.readline()\n",
    "        line = f1.readline()\n",
    "        while line:\n",
    "            line = line.rstrip(\"\\r\\n\").split(',')\n",
    "            #print(line)\n",
    "            tmp = []\n",
    "            for i in range(4):\n",
    "                tmp.append(feature_dict.get(feature_list[i]).index(line[i+1]))\n",
    "            y_train.append(label_list.index(line[5]))\n",
    "            x_train.append(tmp)\n",
    "            line = f1.readline()\n",
    "    return np.array(x_train), np.array(y_train)\n",
    "\n",
    "x_train1, y_train1 = load_data1('Watermelon-train1.csv')\n",
    "x_test1, y_test1 = load_data1('Watermelon-test1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0] 1\n",
      "[1 0 1 0] 1\n",
      "[1 0 0 0] 1\n",
      "[0 0 1 0] 1\n",
      "[2 0 0 0] 1\n",
      "[0 1 0 0] 1\n",
      "[1 1 0 1] 1\n",
      "[1 1 0 0] 1\n",
      "[1 1 1 1] 0\n",
      "[0 2 2 0] 0\n",
      "[2 2 2 2] 0\n",
      "[2 0 0 2] 0\n",
      "[0 1 0 1] 0\n",
      "[2 1 1 1] 0\n",
      "[2 0 0 2] 0\n",
      "[0 0 1 1] 0\n"
     ]
    }
   ],
   "source": [
    "for i in range(y_train1.shape[0]):\n",
    "    print(x_train1[i], y_train1[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data2(path):\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    with open(path,'r') as f1:\n",
    "        line = f1.readline()\n",
    "        line = f1.readline()\n",
    "        while line:\n",
    "            line = line.rstrip(\"\\r\\n\").split(',')\n",
    "            #print(line)\n",
    "            tmp = []\n",
    "            for i in range(4):\n",
    "                tmp.append(feature_dict.get(feature_list[i]).index(line[i+1]))\n",
    "            tmp.append(float(line[5]))\n",
    "            y_train.append(label_list.index(line[6]))\n",
    "            x_train.append(tmp)\n",
    "            line = f1.readline()\n",
    "    return np.array(x_train), np.array(y_train)\n",
    "\n",
    "x_train2, y_train2 = load_data2('Watermelon-train2.csv')\n",
    "x_test2, y_test2 = load_data2('Watermelon-test2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基本要求"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算数据集的信息熵\n",
    "def entropy(data):\n",
    "    label = data[:,-1] if data.ndim==2 else data\n",
    "    label_counts = Counter(label)\n",
    "    probs = [count / len(label) for count in label_counts.values()]\n",
    "    ent = -sum(p * np.log2(p) for p in probs if p>0)\n",
    "    return ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算某个特征的信息增益\n",
    "def gain(x, y, feature_index):\n",
    "    original_ent = entropy(y)\n",
    "    if feature_index!=4: # 离散属性值\n",
    "        weighted_ent = 0\n",
    "        values = set(x[:, feature_index])\n",
    "        for value in values:\n",
    "            subset = x[x[:, feature_index] == value]\n",
    "            weighted_ent += len(subset) / len(y) * entropy(subset)\n",
    "        return original_ent - weighted_ent\n",
    "    else: # 连续属性值，排序后二分\n",
    "        sorted_x = x[np.argsort(x[:, feature_index])]\n",
    "        thresholds = [(sorted_x[i, feature_index] + sorted_x[i+1, feature_index]) / 2 for i in range(len(x))]\n",
    "        gains = []\n",
    "        for threshold in thresholds:\n",
    "            less_subset = sorted_x[sorted_x[:, feature_index] <= threshold]\n",
    "            greater_subset = sorted_x[sorted_x[:, feature_index] > threshold]\n",
    "            less_weight = len(less_subset) / len(y)\n",
    "            greater_weight = len(greater_subset) / len(y)\n",
    "            gain = original_ent - (less_weight * entropy(less_subset) + greater_weight * entropy(greater_subset))\n",
    "            gains.append(gain)\n",
    "        best_threshold_idx = np.argmax(gains)\n",
    "        return thresholds[best_threshold_idx], gains[best_threshold_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def id3(x, y, features):\n",
    "    # 如果所有样本属于同一类别，返回叶节点\n",
    "    if len(set(y)) == 1:\n",
    "        return y[0]\n",
    "\n",
    "    # 如果没有特征可用，返回叶节点，选择样本最多的类\n",
    "    if len(features) == 0:\n",
    "        return Counter(y).most_common(1)[0][0]\n",
    "    \n",
    "    # 选择信息增益最大的特征\n",
    "    best_feature = max(features, key=lambda feature: gain(x, y, feature))\n",
    "    print(\"best feature = \",best_feature)\n",
    "    # 构建树\n",
    "    tree = {best_feature : {}}\n",
    "    feature_values = set(x[:, best_feature])\n",
    "    for value in feature_values:\n",
    "        print(\"feature\",best_feature,\"'s value = \",value)\n",
    "        condition = x[:,best_feature] == value\n",
    "        subset_idx = [i for i in range(len(condition)) if condition[i]==True]\n",
    "        x_subset = x[subset_idx]\n",
    "        y_subset = y[subset_idx]\n",
    "        print(\"subset:\")\n",
    "        for i in range(len(subset_idx)):\n",
    "            print(x_subset[i], y_subset[i])\n",
    "        remaining_features = [f for f in features if f!=best_feature]\n",
    "        print(\"remaining features: \",remaining_features)\n",
    "        tree[best_feature][value] = id3(x_subset, y_subset, remaining_features)\n",
    "    print(tree)\n",
    "    print(\"----------------------------------\")\n",
    "    return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "features1 = [0, 1, 2, 3]\n",
    "features2 = [0, 1, 2, 3, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best feature =  3\n",
      "feature 3 's value =  0\n",
      "subset:\n",
      "[0 0 0 0] 1\n",
      "[1 0 1 0] 1\n",
      "[1 0 0 0] 1\n",
      "[0 0 1 0] 1\n",
      "[2 0 0 0] 1\n",
      "[0 1 0 0] 1\n",
      "[1 1 0 0] 1\n",
      "[0 2 2 0] 0\n",
      "remaining features:  [0, 1, 2]\n",
      "best feature =  0\n",
      "feature 0 's value =  0\n",
      "subset:\n",
      "[0 0 0 0] 1\n",
      "[0 0 1 0] 1\n",
      "[0 1 0 0] 1\n",
      "[0 2 2 0] 0\n",
      "remaining features:  [1, 2]\n",
      "best feature =  1\n",
      "feature 1 's value =  0\n",
      "subset:\n",
      "[0 0 0 0] 1\n",
      "[0 0 1 0] 1\n",
      "remaining features:  [2]\n",
      "feature 1 's value =  1\n",
      "subset:\n",
      "[0 1 0 0] 1\n",
      "remaining features:  [2]\n",
      "feature 1 's value =  2\n",
      "subset:\n",
      "[0 2 2 0] 0\n",
      "remaining features:  [2]\n",
      "{1: {0: 1, 1: 1, 2: 0}}\n",
      "----------------------------------\n",
      "feature 0 's value =  1\n",
      "subset:\n",
      "[1 0 1 0] 1\n",
      "[1 0 0 0] 1\n",
      "[1 1 0 0] 1\n",
      "remaining features:  [1, 2]\n",
      "feature 0 's value =  2\n",
      "subset:\n",
      "[2 0 0 0] 1\n",
      "remaining features:  [1, 2]\n",
      "{0: {0: {1: {0: 1, 1: 1, 2: 0}}, 1: 1, 2: 1}}\n",
      "----------------------------------\n",
      "feature 3 's value =  1\n",
      "subset:\n",
      "[1 1 0 1] 1\n",
      "[1 1 1 1] 0\n",
      "[0 1 0 1] 0\n",
      "[2 1 1 1] 0\n",
      "[0 0 1 1] 0\n",
      "remaining features:  [0, 1, 2]\n",
      "best feature =  0\n",
      "feature 0 's value =  0\n",
      "subset:\n",
      "[0 1 0 1] 0\n",
      "[0 0 1 1] 0\n",
      "remaining features:  [1, 2]\n",
      "feature 0 's value =  1\n",
      "subset:\n",
      "[1 1 0 1] 1\n",
      "[1 1 1 1] 0\n",
      "remaining features:  [1, 2]\n",
      "best feature =  1\n",
      "feature 1 's value =  1\n",
      "subset:\n",
      "[1 1 0 1] 1\n",
      "[1 1 1 1] 0\n",
      "remaining features:  [2]\n",
      "best feature =  2\n",
      "feature 2 's value =  0\n",
      "subset:\n",
      "[1 1 0 1] 1\n",
      "remaining features:  []\n",
      "feature 2 's value =  1\n",
      "subset:\n",
      "[1 1 1 1] 0\n",
      "remaining features:  []\n",
      "{2: {0: 1, 1: 0}}\n",
      "----------------------------------\n",
      "{1: {1: {2: {0: 1, 1: 0}}}}\n",
      "----------------------------------\n",
      "feature 0 's value =  2\n",
      "subset:\n",
      "[2 1 1 1] 0\n",
      "remaining features:  [1, 2]\n",
      "{0: {0: 0, 1: {1: {1: {2: {0: 1, 1: 0}}}}, 2: 0}}\n",
      "----------------------------------\n",
      "feature 3 's value =  2\n",
      "subset:\n",
      "[2 2 2 2] 0\n",
      "[2 0 0 2] 0\n",
      "[2 0 0 2] 0\n",
      "remaining features:  [0, 1, 2]\n",
      "{3: {0: {0: {0: {1: {0: 1, 1: 1, 2: 0}}, 1: 1, 2: 1}}, 1: {0: {0: 0, 1: {1: {1: {2: {0: 1, 1: 0}}}}, 2: 0}}, 2: 0}}\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "id3_tree = id3(x_train1, y_train1, features1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(tree, sample):\n",
    "    if isinstance(tree, dict): # 如果当前节点是一个dict，表示还需要往下看\n",
    "        feature_index = list(tree.keys())[0] # 获取当前节点的划分特征索引\n",
    "        feature_value = sample[feature_index] # 样本在该特征上的取值\n",
    "        sub_tree = tree[feature_index][feature_value] # 根据取值获取子树\n",
    "        return predict(sub_tree, sample) # 继续判断下一个节点\n",
    "    else:\n",
    "        return tree # 当前节点是叶节点，直接返回预测类别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID3 树的预测结果有 7/10 个正确，accuracy = 0.70\n"
     ]
    }
   ],
   "source": [
    "correct1 = 0\n",
    "for i in range(len(y_test1)):\n",
    "    id3_predict = predict(id3_tree, x_test1[i])\n",
    "    if id3_predict==y_test1[i]:\n",
    "        correct1 += 1\n",
    "accuracy1 = correct1 / len(y_test1)\n",
    "print(\"ID3 树的预测结果有 %d/%d 个正确，accuracy = %.2f\" % (correct1, len(y_test1), accuracy1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 中级要求"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gain_ratio(x, y, feature_index, split_values=None):\n",
    "    original_ent = entropy(y)\n",
    "    values = set(x[:, feature_index]) if split_values is None else split_values\n",
    "    intrinsic_val = 0\n",
    "    weighted_ent = 0\n",
    "    for value in set(x[:, feature_index]):\n",
    "        subset = x[x[:, feature_index] == value]\n",
    "        intrinsic_val -= len(subset) / len(y) * np.log2(len(subset)) / len(y)\n",
    "    if feature_index!=4:\n",
    "        for value in values:\n",
    "            subset = x[x[:, feature_index] == value]\n",
    "            weighted_ent += len(subset) / len(y) * entropy(subset)\n",
    "        gain = original_ent - weighted_ent\n",
    "        gain_ratio = gain / intrinsic_val if intrinsic_val!=0 else 0\n",
    "    else:\n",
    "        sorted_x = x[np.argsort(x[:, feature_index])]\n",
    "        gains = []\n",
    "        for value in values:\n",
    "            less_subset = sorted_x[sorted_x[:, feature_index] <= value]\n",
    "            greater_subset = sorted_x[sorted_x[:, feature_index] > value]\n",
    "            less_weight = len(less_subset) / len(y)\n",
    "            greater_weight = len(greater_subset) / len(y)\n",
    "            gain = original_ent - (less_weight * entropy(less_subset) + greater_weight * entropy(greater_subset))\n",
    "            gains.append(gain)\n",
    "        gain_ratio = max(gains) / intrinsic_val if intrinsic_val!=0 else 0\n",
    "    return gain_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_split(x, y, features):\n",
    "    best_feature = None\n",
    "    best_split_values = None\n",
    "    best_gain_ratio = -1\n",
    "    for feature in features:\n",
    "        unique_values = set(x[:, feature])\n",
    "        if len(unique_values) == 1: # 跳过只有一个取值的特征\n",
    "            continue\n",
    "        if feature!=4: # 离散特征\n",
    "            cur_gain_ratio = gain_ratio(x, y, feature)\n",
    "            split_values = None\n",
    "        else: # 连续特征\n",
    "            sorted_values = sorted(unique_values)\n",
    "            split_values = [(sorted_values[i] + sorted_values[i+1]) / 2 for i in range(len(sorted_values)-1)]\n",
    "            cur_gain_ratio = max((gain_ratio(x, y, feature, split_values=[value]) for value in split_values),\n",
    "                                 default=0)\n",
    "        if cur_gain_ratio > best_gain_ratio:\n",
    "            best_gain_ratio = cur_gain_ratio\n",
    "            best_feature = feature\n",
    "            best_split_values = split_values if split_values is not None else None\n",
    "    return best_feature, best_split_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def C4_5(x, y, features):\n",
    "    # 如果所有样本属于同一类别，返回叶节点\n",
    "    if len(set(y)) == 1:\n",
    "        return y[0]\n",
    "\n",
    "    # 如果没有特征可用，返回叶节点，选择样本最多的类\n",
    "    if len(features) == 0:\n",
    "        return Counter(y).most_common(1)[0][0]\n",
    "    \n",
    "    # 选择信息增益最大的特征\n",
    "    best_feature, best_split_values = find_best_split(x, y, features)\n",
    "    if best_feature is None:\n",
    "        return Counter(y).most_common(1)[0][0]\n",
    "    print(\"best feature = \",best_feature)\n",
    "\n",
    "    # 构建树\n",
    "    tree = {best_feature: {}}\n",
    "\n",
    "    if best_split_values is None: # 离散特征\n",
    "        for value in set(x[:, best_feature]):\n",
    "            print(\"feature\",best_feature,\"'s split value = \",value)\n",
    "            condition = x[:,best_feature] == value\n",
    "            subset_idx = [i for i in range(len(condition)) if condition[i]==True]\n",
    "            x_subset = x[subset_idx]\n",
    "            y_subset = y[subset_idx]\n",
    "            print(\"subset:\")\n",
    "            for i in range(len(subset_idx)):\n",
    "                print(x_subset[i], y_subset[i])\n",
    "            remaining_features = [f for f in features if f!=best_feature]\n",
    "            print(\"remaining features: \",remaining_features)\n",
    "            tree[best_feature][value] = C4_5(x_subset, y_subset, remaining_features)\n",
    "    else: # 连续特征\n",
    "        for value in best_split_values:\n",
    "            print(\"feature\",best_feature,\"'s split value = \",value)\n",
    "            condition = x[:,best_feature] <= value\n",
    "            subset_idx = [i for i in range(len(condition)) if condition[i]==True]\n",
    "            x_subset = x[subset_idx]\n",
    "            y_subset = y[subset_idx]\n",
    "            print(\"subset:\")\n",
    "            for i in range(len(subset_idx)):\n",
    "                print(x_subset[i], y_subset[i])\n",
    "            remaining_features = [f for f in features if f!=best_feature]\n",
    "            print(\"remaining features: \",remaining_features)\n",
    "            tree[best_feature][f'<= {best_split_values[-1]}'] = C4_5(x_subset, y_subset, remaining_features)\n",
    "        greater_condition = x[:,best_feature] > best_split_values[-1]\n",
    "        subset_idx = [i for i in range(len(greater_condition)) if greater_condition[i]==True]\n",
    "        x_subset = x[subset_idx]\n",
    "        y_subset = y[subset_idx]\n",
    "        print(\"subset:\")\n",
    "        for i in range(len(subset_idx)):\n",
    "            print(x_subset[i], y_subset[i])\n",
    "        remaining_features = [f for f in features if f!=best_feature]\n",
    "        print(\"remaining features: \",remaining_features)\n",
    "        tree[best_feature][f'> {best_split_values[-1]}'] = C4_5(x_subset, y_subset, remaining_features)\n",
    "\n",
    "    print(tree)\n",
    "    print(\"----------------------------------\")\n",
    "    return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best feature =  2\n",
      "feature 2 's split value =  0.0\n",
      "subset:\n",
      "[0.    0.    0.    0.    0.697] 1\n",
      "[1.    0.    0.    0.    0.634] 1\n",
      "[2.    0.    0.    0.    0.556] 1\n",
      "[0.    1.    0.    0.    0.403] 1\n",
      "[1.    1.    0.    1.    0.481] 1\n",
      "[1.    1.    0.    0.    0.437] 1\n",
      "[2.    0.    0.    2.    0.343] 0\n",
      "[0.    1.    0.    1.    0.639] 0\n",
      "[1.   1.   0.   0.   0.36] 0\n",
      "[2.    0.    0.    2.    0.593] 0\n",
      "remaining features:  [0, 1, 3, 4]\n",
      "best feature =  1\n",
      "feature 1 's split value =  0.0\n",
      "subset:\n",
      "[0.    0.    0.    0.    0.697] 1\n",
      "[1.    0.    0.    0.    0.634] 1\n",
      "[2.    0.    0.    0.    0.556] 1\n",
      "[2.    0.    0.    2.    0.343] 0\n",
      "[2.    0.    0.    2.    0.593] 0\n",
      "remaining features:  [0, 3, 4]\n",
      "best feature =  3\n",
      "feature 3 's split value =  0.0\n",
      "subset:\n",
      "[0.    0.    0.    0.    0.697] 1\n",
      "[1.    0.    0.    0.    0.634] 1\n",
      "[2.    0.    0.    0.    0.556] 1\n",
      "remaining features:  [0, 4]\n",
      "feature 3 's split value =  2.0\n",
      "subset:\n",
      "[2.    0.    0.    2.    0.343] 0\n",
      "[2.    0.    0.    2.    0.593] 0\n",
      "remaining features:  [0, 4]\n",
      "{3: {0.0: 1, 2.0: 0}}\n",
      "----------------------------------\n",
      "feature 1 's split value =  1.0\n",
      "subset:\n",
      "[0.    1.    0.    0.    0.403] 1\n",
      "[1.    1.    0.    1.    0.481] 1\n",
      "[1.    1.    0.    0.    0.437] 1\n",
      "[0.    1.    0.    1.    0.639] 0\n",
      "[1.   1.   0.   0.   0.36] 0\n",
      "remaining features:  [0, 3, 4]\n",
      "best feature =  0\n",
      "feature 0 's split value =  0.0\n",
      "subset:\n",
      "[0.    1.    0.    0.    0.403] 1\n",
      "[0.    1.    0.    1.    0.639] 0\n",
      "remaining features:  [3, 4]\n",
      "best feature =  3\n",
      "feature 3 's split value =  0.0\n",
      "subset:\n",
      "[0.    1.    0.    0.    0.403] 1\n",
      "remaining features:  [4]\n",
      "feature 3 's split value =  1.0\n",
      "subset:\n",
      "[0.    1.    0.    1.    0.639] 0\n",
      "remaining features:  [4]\n",
      "{3: {0.0: 1, 1.0: 0}}\n",
      "----------------------------------\n",
      "feature 0 's split value =  1.0\n",
      "subset:\n",
      "[1.    1.    0.    1.    0.481] 1\n",
      "[1.    1.    0.    0.    0.437] 1\n",
      "[1.   1.   0.   0.   0.36] 0\n",
      "remaining features:  [3, 4]\n",
      "best feature =  4\n",
      "feature 4 's split value =  0.39849999999999997\n",
      "subset:\n",
      "[1.   1.   0.   0.   0.36] 0\n",
      "remaining features:  [3]\n",
      "feature 4 's split value =  0.45899999999999996\n",
      "subset:\n",
      "[1.    1.    0.    0.    0.437] 1\n",
      "[1.   1.   0.   0.   0.36] 0\n",
      "remaining features:  [3]\n",
      "subset:\n",
      "[1.    1.    0.    1.    0.481] 1\n",
      "remaining features:  [3]\n",
      "{4: {'<= 0.45899999999999996': 1, '> 0.45899999999999996': 1}}\n",
      "----------------------------------\n",
      "{0: {0.0: {3: {0.0: 1, 1.0: 0}}, 1.0: {4: {'<= 0.45899999999999996': 1, '> 0.45899999999999996': 1}}}}\n",
      "----------------------------------\n",
      "{1: {0.0: {3: {0.0: 1, 2.0: 0}}, 1.0: {0: {0.0: {3: {0.0: 1, 1.0: 0}}, 1.0: {4: {'<= 0.45899999999999996': 1, '> 0.45899999999999996': 1}}}}}}\n",
      "----------------------------------\n",
      "feature 2 's split value =  1.0\n",
      "subset:\n",
      "[1.    0.    1.    0.    0.774] 1\n",
      "[0.    0.    1.    0.    0.608] 1\n",
      "[1.    1.    1.    1.    0.666] 0\n",
      "[2.    1.    1.    1.    0.657] 0\n",
      "[0.    0.    1.    1.    0.719] 0\n",
      "remaining features:  [0, 1, 3, 4]\n",
      "best feature =  1\n",
      "feature 1 's split value =  0.0\n",
      "subset:\n",
      "[1.    0.    1.    0.    0.774] 1\n",
      "[0.    0.    1.    0.    0.608] 1\n",
      "[0.    0.    1.    1.    0.719] 0\n",
      "remaining features:  [0, 3, 4]\n",
      "best feature =  4\n",
      "feature 4 's split value =  0.6635\n",
      "subset:\n",
      "[0.    0.    1.    0.    0.608] 1\n",
      "remaining features:  [0, 3]\n",
      "feature 4 's split value =  0.7464999999999999\n",
      "subset:\n",
      "[0.    0.    1.    0.    0.608] 1\n",
      "[0.    0.    1.    1.    0.719] 0\n",
      "remaining features:  [0, 3]\n",
      "best feature =  3\n",
      "feature 3 's split value =  0.0\n",
      "subset:\n",
      "[0.    0.    1.    0.    0.608] 1\n",
      "remaining features:  [0]\n",
      "feature 3 's split value =  1.0\n",
      "subset:\n",
      "[0.    0.    1.    1.    0.719] 0\n",
      "remaining features:  [0]\n",
      "{3: {0.0: 1, 1.0: 0}}\n",
      "----------------------------------\n",
      "subset:\n",
      "[1.    0.    1.    0.    0.774] 1\n",
      "remaining features:  [0, 3]\n",
      "{4: {'<= 0.7464999999999999': {3: {0.0: 1, 1.0: 0}}, '> 0.7464999999999999': 1}}\n",
      "----------------------------------\n",
      "feature 1 's split value =  1.0\n",
      "subset:\n",
      "[1.    1.    1.    1.    0.666] 0\n",
      "[2.    1.    1.    1.    0.657] 0\n",
      "remaining features:  [0, 3, 4]\n",
      "{1: {0.0: {4: {'<= 0.7464999999999999': {3: {0.0: 1, 1.0: 0}}, '> 0.7464999999999999': 1}}, 1.0: 0}}\n",
      "----------------------------------\n",
      "feature 2 's split value =  2.0\n",
      "subset:\n",
      "[0.    2.    2.    0.    0.243] 0\n",
      "[2.    2.    2.    2.    0.245] 0\n",
      "remaining features:  [0, 1, 3, 4]\n",
      "{2: {0.0: {1: {0.0: {3: {0.0: 1, 2.0: 0}}, 1.0: {0: {0.0: {3: {0.0: 1, 1.0: 0}}, 1.0: {4: {'<= 0.45899999999999996': 1, '> 0.45899999999999996': 1}}}}}}, 1.0: {1: {0.0: {4: {'<= 0.7464999999999999': {3: {0.0: 1, 1.0: 0}}, '> 0.7464999999999999': 1}}, 1.0: 0}}, 2.0: 0}}\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "C4_5_tree = C4_5(x_train2, y_train2, features2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_C4_5(tree, sample):\n",
    "    if isinstance(tree, dict): # 如果当前节点是一个dict，表示还需要往下看\n",
    "        feature_index = list(tree.keys())[0] # 获取当前节点的划分特征索引\n",
    "        feature_value = sample[feature_index] # 样本在该特征上的取值\n",
    "        for condition, subtree in tree[feature_index].items():\n",
    "            if isinstance(condition, str):\n",
    "                condition_value = float(condition.split(' ')[1]) if ' ' in condition else float('inf')\n",
    "                if(condition.startswith('<=') and feature_value <= condition_value) or \\\n",
    "            (condition_value.startswith('>') and feature_value > condition_value):\n",
    "                    return predict_C4_5(subtree, sample)\n",
    "            else:\n",
    "                condition_value = condition\n",
    "                if feature_value == condition_value:\n",
    "                    return predict_C4_5(subtree, sample)\n",
    "    else:\n",
    "        return tree # 当前节点是叶节点，直接返回预测类别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C4.5 树的预测结果有 4/5 个正确，accuracy = 0.80\n"
     ]
    }
   ],
   "source": [
    "correct2 = 0\n",
    "for i in range(len(y_test2)):\n",
    "    C4_5_predict = predict_C4_5(C4_5_tree, x_test2[i])\n",
    "    if C4_5_predict==y_test2[i]:\n",
    "        correct2 += 1\n",
    "accuracy2 = correct2 / len(y_test2)\n",
    "print(\"C4.5 树的预测结果有 %d/%d 个正确，accuracy = %.2f\" % (correct2, len(y_test2), accuracy2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 高级要求"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分别对基本要求和中级要求里的 ID3 和 C4.5 决策树进行基于深度剪枝的预剪枝，评估在不同最大深度时模型的性能："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "def id3_pre_pruning(x, y, features, max_depth = None, depth=0):\n",
    "    # 如果所有样本属于同一类别，返回叶节点\n",
    "    if len(set(y)) == 1:\n",
    "        return y[0]\n",
    "\n",
    "    # 如果没有特征可用，返回叶节点，选择样本最多的类\n",
    "    if len(features) == 0:\n",
    "        return Counter(y).most_common(1)[0][0]\n",
    "    \n",
    "    # 达到深度限制，返回样本最多的类\n",
    "    if max_depth is not None and depth==max_depth:\n",
    "        return Counter(y).most_common(1)[0][0]\n",
    "    \n",
    "    # 选择信息增益最大的特征\n",
    "    best_feature = max(features, key=lambda feature: gain(x, y, feature))\n",
    "    # 构建树\n",
    "    tree = {best_feature : {}}\n",
    "    feature_values = set(x[:, best_feature])\n",
    "    for value in feature_values:\n",
    "        condition = x[:,best_feature] == value\n",
    "        subset_idx = [i for i in range(len(condition)) if condition[i]==True]\n",
    "        x_subset = x[subset_idx]\n",
    "        y_subset = y[subset_idx]\n",
    "        remaining_features = [f for f in features if f!=best_feature]\n",
    "        tree[best_feature][value] = id3_pre_pruning(x_subset, y_subset, remaining_features, max_depth, depth+1)\n",
    "    return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{3: {0: 1, 1: 0, 2: 0}}\n",
      "预剪枝 ID3 树在最大深度 1 下的预测结果有 8/10 个正确，accuracy = 0.80\n",
      "\n",
      "{3: {0: {0: {0: 1, 1: 1, 2: 1}}, 1: {0: {0: 0, 1: 1, 2: 0}}, 2: 0}}\n",
      "预剪枝 ID3 树在最大深度 2 下的预测结果有 6/10 个正确，accuracy = 0.60\n",
      "\n",
      "{3: {0: {0: {0: {1: {0: 1, 1: 1, 2: 0}}, 1: 1, 2: 1}}, 1: {0: {0: 0, 1: {1: {1: 1}}, 2: 0}}, 2: 0}}\n",
      "预剪枝 ID3 树在最大深度 3 下的预测结果有 6/10 个正确，accuracy = 0.60\n",
      "\n",
      "{3: {0: {0: {0: {1: {0: 1, 1: 1, 2: 0}}, 1: 1, 2: 1}}, 1: {0: {0: 0, 1: {1: {1: {2: {0: 1, 1: 0}}}}, 2: 0}}, 2: 0}}\n",
      "预剪枝 ID3 树在最大深度 4 下的预测结果有 7/10 个正确，accuracy = 0.70\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for d in range(len(features1)):\n",
    "    id3_pre_pruning_tree = id3_pre_pruning(x_train1, y_train1, features1, d+1)\n",
    "    print(id3_pre_pruning_tree)\n",
    "    correct3 = 0\n",
    "    for i in range(len(y_test1)):\n",
    "        id3_pre_pruning_predict = predict(id3_pre_pruning_tree, x_test1[i])\n",
    "        if id3_pre_pruning_predict==y_test1[i]:\n",
    "            correct3 += 1\n",
    "    accuracy3 = correct3 / len(y_test1)\n",
    "    print(\"预剪枝 ID3 树在最大深度 %d 下的预测结果有 %d/%d 个正确，accuracy = %.2f\\n\" % (d+1, correct3, len(y_test1), accuracy3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "def C4_5_pre_pruning(x, y, features, max_depth=None, depth=0):\n",
    "    # 如果所有样本属于同一类别，返回叶节点\n",
    "    if len(set(y)) == 1:\n",
    "        return y[0]\n",
    "\n",
    "    # 如果没有特征可用，返回叶节点，选择样本最多的类\n",
    "    if len(features) == 0:\n",
    "        return Counter(y).most_common(1)[0][0]\n",
    "    \n",
    "    # 达到深度限制，返回样本最多的类\n",
    "    if max_depth is not None and depth==max_depth:\n",
    "        return Counter(y).most_common(1)[0][0]\n",
    "    \n",
    "    # 选择信息增益最大的特征\n",
    "    best_feature, best_split_values = find_best_split(x, y, features)\n",
    "    if best_feature is None:\n",
    "        return Counter(y).most_common(1)[0][0]\n",
    "    #print(\"best feature = \",best_feature)\n",
    "\n",
    "    # 构建树\n",
    "    tree = {best_feature: {}}\n",
    "\n",
    "    if best_split_values is None: # 离散特征\n",
    "        for value in set(x[:, best_feature]):\n",
    "            condition = x[:,best_feature] == value\n",
    "            subset_idx = [i for i in range(len(condition)) if condition[i]==True]\n",
    "            x_subset = x[subset_idx]\n",
    "            y_subset = y[subset_idx]\n",
    "            remaining_features = [f for f in features if f!=best_feature]\n",
    "            tree[best_feature][value] = C4_5_pre_pruning(x_subset, y_subset, remaining_features, max_depth, depth+1)\n",
    "    else: # 连续特征\n",
    "        for value in best_split_values:\n",
    "            condition = x[:,best_feature] <= value\n",
    "            subset_idx = [i for i in range(len(condition)) if condition[i]==True]\n",
    "            x_subset = x[subset_idx]\n",
    "            y_subset = y[subset_idx]\n",
    "            remaining_features = [f for f in features if f!=best_feature]\n",
    "            tree[best_feature][f'<= {best_split_values[-1]}'] = C4_5_pre_pruning(x_subset, y_subset, remaining_features, max_depth, depth+1)\n",
    "        greater_condition = x[:,best_feature] > best_split_values[-1]\n",
    "        subset_idx = [i for i in range(len(greater_condition)) if greater_condition[i]==True]\n",
    "        x_subset = x[subset_idx]\n",
    "        y_subset = y[subset_idx]\n",
    "        remaining_features = [f for f in features if f!=best_feature]\n",
    "        tree[best_feature][f'> {best_split_values[-1]}'] = C4_5_pre_pruning(x_subset, y_subset, remaining_features, max_depth, depth+1)\n",
    "\n",
    "    return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{2: {0.0: 1, 1.0: 0, 2.0: 0}}\n",
      "预剪枝 C4.5 树在最大深度 1 下的的预测结果有 5/5 个正确，accuracy = 1.00\n",
      "\n",
      "{2: {0.0: {1: {0.0: 1, 1.0: 1}}, 1.0: {1: {0.0: 1, 1.0: 0}}, 2.0: 0}}\n",
      "预剪枝 C4.5 树在最大深度 2 下的的预测结果有 5/5 个正确，accuracy = 1.00\n",
      "\n",
      "{2: {0.0: {1: {0.0: {3: {0.0: 1, 2.0: 0}}, 1.0: {0: {0.0: 1, 1.0: 1}}}}, 1.0: {1: {0.0: {4: {'<= 0.7464999999999999': 1, '> 0.7464999999999999': 1}}, 1.0: 0}}, 2.0: 0}}\n",
      "预剪枝 C4.5 树在最大深度 3 下的的预测结果有 5/5 个正确，accuracy = 1.00\n",
      "\n",
      "{2: {0.0: {1: {0.0: {3: {0.0: 1, 2.0: 0}}, 1.0: {0: {0.0: {3: {0.0: 1, 1.0: 0}}, 1.0: {4: {'<= 0.45899999999999996': 1, '> 0.45899999999999996': 1}}}}}}, 1.0: {1: {0.0: {4: {'<= 0.7464999999999999': {3: {0.0: 1, 1.0: 0}}, '> 0.7464999999999999': 1}}, 1.0: 0}}, 2.0: 0}}\n",
      "预剪枝 C4.5 树在最大深度 4 下的的预测结果有 4/5 个正确，accuracy = 0.80\n",
      "\n",
      "{2: {0.0: {1: {0.0: {3: {0.0: 1, 2.0: 0}}, 1.0: {0: {0.0: {3: {0.0: 1, 1.0: 0}}, 1.0: {4: {'<= 0.45899999999999996': 1, '> 0.45899999999999996': 1}}}}}}, 1.0: {1: {0.0: {4: {'<= 0.7464999999999999': {3: {0.0: 1, 1.0: 0}}, '> 0.7464999999999999': 1}}, 1.0: 0}}, 2.0: 0}}\n",
      "预剪枝 C4.5 树在最大深度 5 下的的预测结果有 4/5 个正确，accuracy = 0.80\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for d in range(len(features2)):\n",
    "    C4_5_pre_pruning_tree = C4_5_pre_pruning(x_train2, y_train2, features2, d+1)\n",
    "    print(C4_5_pre_pruning_tree)\n",
    "    correct4 = 0\n",
    "    for i in range(len(y_test2)):\n",
    "        C4_5_pre_pruning_predict = predict_C4_5(C4_5_pre_pruning_tree, x_test2[i])\n",
    "        if C4_5_pre_pruning_predict==y_test2[i]:\n",
    "            correct4 += 1\n",
    "    accuracy4 = correct4 / len(y_test2)\n",
    "    print(\"预剪枝 C4.5 树在最大深度 %d 下的的预测结果有 %d/%d 个正确，accuracy = %.2f\\n\" % (d+1, correct4, len(y_test2), accuracy4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，进行预剪枝后，两种模型的性能在最大深度较小时都有不同程度的提升。这是因为决策树考虑的完全是如何提高训练数据分类的正确性，导致模型很有可能过拟合，对噪声数据非常敏感；基于深度的预剪枝可以提前停止树的增长，提高模型的泛化性能。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "temp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
